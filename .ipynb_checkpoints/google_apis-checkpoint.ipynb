{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import io\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"path/key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks:\n",
      "Description :  Taj Mahal\n",
      "Latitude 27.174698469698683\n",
      "Longitude 78.042073\n"
     ]
    }
   ],
   "source": [
    "path = 'path/taj.jpg'\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "with io.open(path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "landmarks = response.landmark_annotations\n",
    "print('Landmarks:')\n",
    "\n",
    "for landmark in landmarks:\n",
    "    print(\"Description : \",landmark.description)\n",
    "    for location in landmark.locations:\n",
    "        lat_lng = location.lat_lng\n",
    "        print('Latitude {}'.format(lat_lng.latitude))\n",
    "        print('Longitude {}'.format(lat_lng.longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Properties\n",
    "The ColorInfo field does not carry information about the absolute color space that should be used to interpret the RGB value (e.g. sRGB, Adobe RGB, DCI-P3, BT.2020, etc.). By default, applications should assume the sRGB color space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties:\n",
      "fraction: 0.05480984225869179\n",
      "\tr: 127.0\n",
      "\tg: 115.0\n",
      "\tb: 121.0\n",
      "\ta: \n",
      "fraction: 0.07614253461360931\n",
      "\tr: 95.0\n",
      "\tg: 85.0\n",
      "\tb: 93.0\n",
      "\ta: \n",
      "fraction: 0.03147970512509346\n",
      "\tr: 161.0\n",
      "\tg: 149.0\n",
      "\tb: 154.0\n",
      "\ta: \n",
      "fraction: 0.06032278761267662\n",
      "\tr: 155.0\n",
      "\tg: 157.0\n",
      "\tb: 181.0\n",
      "\ta: \n",
      "fraction: 0.15164589881896973\n",
      "\tr: 181.0\n",
      "\tg: 187.0\n",
      "\tb: 216.0\n",
      "\ta: \n",
      "fraction: 0.019734740257263184\n",
      "\tr: 204.0\n",
      "\tg: 191.0\n",
      "\tb: 195.0\n",
      "\ta: \n",
      "fraction: 0.011744966730475426\n",
      "\tr: 86.0\n",
      "\tg: 84.0\n",
      "\tb: 104.0\n",
      "\ta: \n",
      "fraction: 0.017178012058138847\n",
      "\tr: 117.0\n",
      "\tg: 116.0\n",
      "\tb: 139.0\n",
      "\ta: \n",
      "fraction: 0.029322467744350433\n",
      "\tr: 103.0\n",
      "\tg: 82.0\n",
      "\tb: 94.0\n",
      "\ta: \n",
      "fraction: 0.0069511025212705135\n",
      "\tr: 131.0\n",
      "\tg: 111.0\n",
      "\tb: 122.0\n",
      "\ta: \n"
     ]
    }
   ],
   "source": [
    "response = client.image_properties(image=image)\n",
    "props = response.image_properties_annotation\n",
    "print('Properties:')\n",
    "\n",
    "for color in props.dominant_colors.colors:\n",
    "    print('fraction: {}'.format(color.pixel_fraction))\n",
    "    print('\\tr: {}'.format(color.color.red))\n",
    "    print('\\tg: {}'.format(color.color.green))\n",
    "    print('\\tb: {}'.format(color.color.blue))\n",
    "    print('\\ta: {}'.format(color.color.alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Moderation\n",
    "Safe Search Detection detects explicit content such as adult content or violent content within an image. This feature uses five categories (adult, spoof, medical, violence, and racy) and returns the likelihood that each is present in a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe search:\n",
      "adult: VERY_UNLIKELY\n",
      "medical: POSSIBLE\n",
      "spoofed: VERY_UNLIKELY\n",
      "violence: LIKELY\n",
      "racy: UNLIKELY\n"
     ]
    }
   ],
   "source": [
    "path = 'path/brain.jpg'\n",
    "with io.open(path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.safe_search_detection(image=image)\n",
    "safe = response.safe_search_annotation\n",
    "\n",
    "# Names of likelihood from google.cloud.vision.enums\n",
    "likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE',\n",
    "                   'LIKELY', 'VERY_LIKELY')\n",
    "print('Safe search:')\n",
    "\n",
    "print('adult: {}'.format(likelihood_name[safe.adult]))\n",
    "print('medical: {}'.format(likelihood_name[safe.medical]))\n",
    "print('spoofed: {}'.format(likelihood_name[safe.spoof]))\n",
    "print('violence: {}'.format(likelihood_name[safe.violence]))\n",
    "print('racy: {}'.format(likelihood_name[safe.racy]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion\n",
    "Only anger, sorrow and joy\n",
    "This does face detection as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces:\n",
      "anger: VERY_UNLIKELY\n",
      "joy: VERY_LIKELY\n",
      "surprise: VERY_UNLIKELY\n"
     ]
    }
   ],
   "source": [
    "path = 'path/happy.jpg'\n",
    "with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.face_detection(image=image)\n",
    "faces = response.face_annotations\n",
    "\n",
    "# Names of likelihood from google.cloud.vision.enums\n",
    "likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE',\n",
    "                   'LIKELY', 'VERY_LIKELY')\n",
    "print('Faces:')\n",
    "\n",
    "for face in faces:\n",
    "    print('anger: {}'.format(likelihood_name[face.anger_likelihood]))\n",
    "    print('joy: {}'.format(likelihood_name[face.joy_likelihood]))\n",
    "    print('surprise: {}'.format(likelihood_name[face.surprise_likelihood]))\n",
    "\n",
    "    vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                for vertex in face.bounding_poly.vertices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Classification\n",
    "Actually an object localizer, we can use just the classification aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects found: 1\n",
      "\n",
      "Tiger (confidence: 0.9640637636184692)\n"
     ]
    }
   ],
   "source": [
    "path = 'path/tiger.jpg'\n",
    "with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "objects = client.object_localization(image=image).localized_object_annotations\n",
    "\n",
    "print('Number of objects found: {}'.format(len(objects)))\n",
    "for object_ in objects:\n",
    "    print('\\n{} (confidence: {})'.format(object_.name, object_.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Celebrity \n",
    "Needs a seperate form, waiting for permission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: except the definition of creativity is production of something original and useful and it is commonly occurs on the play an important role in enhancing according to new research creativity isn't about Freedom From Concrete facts\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import speech_v1\n",
    "from google.cloud.speech_v1 import enums\n",
    "import io\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"D:/NYU_MS/Sem3/CloudML/key.json\"\n",
    "\n",
    "local_file_path = 'D:/NYU_MS/Sem3/CloudML/Adveserial-Attacks/audio-dataset/audio/00.wav'\n",
    "\n",
    "client = speech_v1.SpeechClient()\n",
    "\n",
    "# local_file_path = 'resources/brooklyn_bridge.raw'\n",
    "\n",
    "# The language of the supplied audio\n",
    "language_code = \"en-US\"\n",
    "\n",
    "# Sample rate in Hertz of the audio data sent\n",
    "sample_rate_hertz = 48000\n",
    "\n",
    "# Encoding of audio data sent. This sample sets this explicitly.\n",
    "# This field is optional for FLAC and WAV audio formats.\n",
    "encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "config = {\n",
    "    \"language_code\": language_code,\n",
    "    \"sample_rate_hertz\": sample_rate_hertz,\n",
    "    \"encoding\": encoding,\n",
    "}\n",
    "with io.open(local_file_path, \"rb\") as f:\n",
    "    content = f.read()\n",
    "audio = {\"content\": content}\n",
    "\n",
    "response = client.recognize(config, audio)\n",
    "for result in response.results:\n",
    "    # First alternative is the most probable result\n",
    "    alternative = result.alternatives[0]\n",
    "    print(u\"Transcript: {}\".format(alternative.transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
