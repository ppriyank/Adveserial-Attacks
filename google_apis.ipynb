{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import io\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"path/key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks:\n",
      "Description :  Taj Mahal\n",
      "Latitude 27.174698469698683\n",
      "Longitude 78.042073\n"
     ]
    }
   ],
   "source": [
    "path = 'path/taj.jpg'\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "with io.open(path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "landmarks = response.landmark_annotations\n",
    "print('Landmarks:')\n",
    "\n",
    "for landmark in landmarks:\n",
    "    print(\"Description : \",landmark.description)\n",
    "    for location in landmark.locations:\n",
    "        lat_lng = location.lat_lng\n",
    "        print('Latitude {}'.format(lat_lng.latitude))\n",
    "        print('Longitude {}'.format(lat_lng.longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Properties\n",
    "The ColorInfo field does not carry information about the absolute color space that should be used to interpret the RGB value (e.g. sRGB, Adobe RGB, DCI-P3, BT.2020, etc.). By default, applications should assume the sRGB color space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties:\n",
      "fraction: 0.05480984225869179\n",
      "\tr: 127.0\n",
      "\tg: 115.0\n",
      "\tb: 121.0\n",
      "\ta: \n",
      "fraction: 0.07614253461360931\n",
      "\tr: 95.0\n",
      "\tg: 85.0\n",
      "\tb: 93.0\n",
      "\ta: \n",
      "fraction: 0.03147970512509346\n",
      "\tr: 161.0\n",
      "\tg: 149.0\n",
      "\tb: 154.0\n",
      "\ta: \n",
      "fraction: 0.06032278761267662\n",
      "\tr: 155.0\n",
      "\tg: 157.0\n",
      "\tb: 181.0\n",
      "\ta: \n",
      "fraction: 0.15164589881896973\n",
      "\tr: 181.0\n",
      "\tg: 187.0\n",
      "\tb: 216.0\n",
      "\ta: \n",
      "fraction: 0.019734740257263184\n",
      "\tr: 204.0\n",
      "\tg: 191.0\n",
      "\tb: 195.0\n",
      "\ta: \n",
      "fraction: 0.011744966730475426\n",
      "\tr: 86.0\n",
      "\tg: 84.0\n",
      "\tb: 104.0\n",
      "\ta: \n",
      "fraction: 0.017178012058138847\n",
      "\tr: 117.0\n",
      "\tg: 116.0\n",
      "\tb: 139.0\n",
      "\ta: \n",
      "fraction: 0.029322467744350433\n",
      "\tr: 103.0\n",
      "\tg: 82.0\n",
      "\tb: 94.0\n",
      "\ta: \n",
      "fraction: 0.0069511025212705135\n",
      "\tr: 131.0\n",
      "\tg: 111.0\n",
      "\tb: 122.0\n",
      "\ta: \n"
     ]
    }
   ],
   "source": [
    "response = client.image_properties(image=image)\n",
    "props = response.image_properties_annotation\n",
    "print('Properties:')\n",
    "\n",
    "for color in props.dominant_colors.colors:\n",
    "    print('fraction: {}'.format(color.pixel_fraction))\n",
    "    print('\\tr: {}'.format(color.color.red))\n",
    "    print('\\tg: {}'.format(color.color.green))\n",
    "    print('\\tb: {}'.format(color.color.blue))\n",
    "    print('\\ta: {}'.format(color.color.alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Moderation\n",
    "Safe Search Detection detects explicit content such as adult content or violent content within an image. This feature uses five categories (adult, spoof, medical, violence, and racy) and returns the likelihood that each is present in a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe search:\n",
      "adult: VERY_UNLIKELY\n",
      "medical: POSSIBLE\n",
      "spoofed: VERY_UNLIKELY\n",
      "violence: LIKELY\n",
      "racy: UNLIKELY\n"
     ]
    }
   ],
   "source": [
    "path = 'path/brain.jpg'\n",
    "with io.open(path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.safe_search_detection(image=image)\n",
    "safe = response.safe_search_annotation\n",
    "\n",
    "# Names of likelihood from google.cloud.vision.enums\n",
    "likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE',\n",
    "                   'LIKELY', 'VERY_LIKELY')\n",
    "print('Safe search:')\n",
    "\n",
    "print('adult: {}'.format(likelihood_name[safe.adult]))\n",
    "print('medical: {}'.format(likelihood_name[safe.medical]))\n",
    "print('spoofed: {}'.format(likelihood_name[safe.spoof]))\n",
    "print('violence: {}'.format(likelihood_name[safe.violence]))\n",
    "print('racy: {}'.format(likelihood_name[safe.racy]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion\n",
    "Only anger, sorrow and joy\n",
    "This does face detection as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces:\n",
      "anger: VERY_UNLIKELY\n",
      "joy: VERY_LIKELY\n",
      "surprise: VERY_UNLIKELY\n"
     ]
    }
   ],
   "source": [
    "path = 'path/happy.jpg'\n",
    "with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.face_detection(image=image)\n",
    "faces = response.face_annotations\n",
    "\n",
    "# Names of likelihood from google.cloud.vision.enums\n",
    "likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE',\n",
    "                   'LIKELY', 'VERY_LIKELY')\n",
    "print('Faces:')\n",
    "\n",
    "for face in faces:\n",
    "    print('anger: {}'.format(likelihood_name[face.anger_likelihood]))\n",
    "    print('joy: {}'.format(likelihood_name[face.joy_likelihood]))\n",
    "    print('surprise: {}'.format(likelihood_name[face.surprise_likelihood]))\n",
    "\n",
    "    vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                for vertex in face.bounding_poly.vertices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Classification\n",
    "Actually an object localizer, we can use just the classification aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects found: 1\n",
      "\n",
      "Tiger (confidence: 0.9640637636184692)\n"
     ]
    }
   ],
   "source": [
    "path = 'path/tiger.jpg'\n",
    "with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "objects = client.object_localization(image=image).localized_object_annotations\n",
    "\n",
    "print('Number of objects found: {}'.format(len(objects)))\n",
    "for object_ in objects:\n",
    "    print('\\n{} (confidence: {})'.format(object_.name, object_.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Celebrity \n",
    "Needs a seperate form, waiting for permission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 sample_rate_hertz (48000) in RecognitionConfig must either be omitted or match the value in the WAV header ( 44100).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_Rendezvous\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\grpc\\_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    689\u001b[0m                                       wait_for_ready, compression)\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\grpc\\_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_Rendezvous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_Rendezvous\u001b[0m: <_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"sample_rate_hertz (48000) in RecognitionConfig must either be omitted or match the value in the WAV header ( 44100).\"\n\tdebug_error_string = \"{\"created\":\"@1574280700.922000000\",\"description\":\"Error received from peer ipv4:172.217.10.74:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1055,\"grpc_message\":\"sample_rate_hertz (48000) in RecognitionConfig must either be omitted or match the value in the WAV header ( 44100).\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b7112612f27d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# First alternative is the most probable result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\cloud\\speech_v1\\gapic\\speech_client.py\u001b[0m in \u001b[0;36mrecognize\u001b[1;34m(self, config, audio, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloud_speech_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecognizeRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         return self._inner_api_calls[\"recognize\"](\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\api_core\\retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mon_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m             )\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\api_core\\retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\api_core\\timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;34m\"\"\"Wrapped function that adds timeout.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"timeout\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 sample_rate_hertz (48000) in RecognitionConfig must either be omitted or match the value in the WAV header ( 44100)."
     ]
    }
   ],
   "source": [
    "from google.cloud import speech_v1\n",
    "from google.cloud.speech_v1 import enums\n",
    "import io\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"path/key.json\"\n",
    "\n",
    "local_file_path = 'path/00.wav'\n",
    "\n",
    "client = speech_v1.SpeechClient()\n",
    "\n",
    "# local_file_path = 'resources/brooklyn_bridge.raw'\n",
    "\n",
    "# The language of the supplied audio\n",
    "language_code = \"en-US\"\n",
    "\n",
    "# Sample rate in Hertz of the audio data sent\n",
    "sample_rate_hertz = 48000\n",
    "\n",
    "# Encoding of audio data sent. This sample sets this explicitly.\n",
    "# This field is optional for FLAC and WAV audio formats.\n",
    "encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "config = {\n",
    "    \"language_code\": language_code,\n",
    "    \"sample_rate_hertz\": sample_rate_hertz,\n",
    "    \"encoding\": encoding,\n",
    "}\n",
    "with io.open(local_file_path, \"rb\") as f:\n",
    "    content = f.read()\n",
    "audio = {\"content\": content}\n",
    "\n",
    "response = client.recognize(config, audio)\n",
    "for result in response.results:\n",
    "    # First alternative is the most probable result\n",
    "    alternative = result.alternatives[0]\n",
    "    print(u\"Transcript: {}\".format(alternative.transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
